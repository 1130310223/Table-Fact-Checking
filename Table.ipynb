{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('input_file_wenhu.csv', mode='w') as fw:\n",
    "    with open('input_file_shiyang.csv', mode='w') as fs:\n",
    "        with open('input_file_yunkai.csv', mode='w') as fy:\n",
    "            \n",
    "            #output_wenhu = output[:5000]\n",
    "            #fileds = ['url1', 'q1', 'a1', 'url2', 'q2', 'a2', 'url3', 'q3', 'a3', 'url4', 'q4', 'a4', 'url5', 'q5', 'a5']\n",
    "            #writer = csv.DictWriter(fw, fieldnames=fileds)\n",
    "            #writer.writeheader()\n",
    "            #for i in range(0, len(output_wenhu) - 5, 5):\n",
    "            #    writer.writerow({'url1':output_wenhu[i]['url'], 'q1':output_wenhu[i]['q'], 'a1':output_wenhu[i]['a'],\n",
    "            #                    'url2':output_wenhu[i+1]['url'], 'q2':output_wenhu[i+1]['q'], 'a2':output_wenhu[i+1]['a'],\n",
    "            #                    'url3':output_wenhu[i+2]['url'], 'q3':output_wenhu[i+2]['q'], 'a3':output_wenhu[i+2]['a'],\n",
    "            #                    'url4':output_wenhu[i+3]['url'], 'q4':output_wenhu[i+3]['q'], 'a4':output_wenhu[i+3]['a'],\n",
    "            #                    'url5':output_wenhu[i+4]['url'], 'q5':output_wenhu[i+4]['q'], 'a5':output_wenhu[i+4]['a']})\n",
    "            \n",
    "            num = 10\n",
    "            \n",
    "            output_shiyang = output[5000:20000]\n",
    "            fields = []\n",
    "            for i in range(1, num+1):\n",
    "                fields.extend(['url{}'.format(i), 'q{}'.format(i), 'a{}'.format(i)])\n",
    "            writer = csv.DictWriter(fs, fieldnames=fields)\n",
    "            writer.writeheader()\n",
    "            for i in range(0, len(output_shiyang) - num, num):\n",
    "                elem = {}\n",
    "                for j in range(1, num+1):\n",
    "                    elem['url{}'.format(j)] = output_shiyang[i+j-1]['url']\n",
    "                    elem['q{}'.format(j)] = output_shiyang[i+j-1]['q']\n",
    "                    elem['a{}'.format(j)] = output_shiyang[i+j-1]['a']\n",
    "                \n",
    "                writer.writerow(elem) \n",
    "            \n",
    "            \n",
    "            output_yunkai = output[20000:]\n",
    "            fields = []\n",
    "            for i in range(1, num+1):\n",
    "                fields.extend(['url{}'.format(i), 'q{}'.format(i), 'a{}'.format(i)])\n",
    "            writer = csv.DictWriter(fy, fieldnames=fields)\n",
    "            writer.writeheader()\n",
    "            for i in range(0, len(output_yunkai) - num, num):\n",
    "                elem = {}\n",
    "                for j in range(1, num+1):\n",
    "                    elem['url{}'.format(j)] = output_yunkai[i+j-1]['url']\n",
    "                    elem['q{}'.format(j)] = output_yunkai[i+j-1]['q']\n",
    "                    elem['a{}'.format(j)] = output_yunkai[i+j-1]['a']\n",
    "                \n",
    "                writer.writerow(elem) \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(inp):\n",
    "    inp = re.sub(r\"> +\", r\">\", inp)\n",
    "    inp = re.sub(r\" +<\", r\"<\", inp)\n",
    "    inp = re.sub(r\">@\", r\">\", inp)\n",
    "    inp = re.sub(r\"\\*\", r\"\", inp)\n",
    "    inp = re.sub(r\"#\", r\"\", inp)\n",
    "    inp = re.sub(r\"‡\", r\"\", inp)\n",
    "    #inp = re.sub(r\".0([^0-9])\", r\"\\1\", inp)\n",
    "    for month, abb in [(\"January\", \"Jan\"), (\"February\", \"Feb\"), (\"March\", \"March\"), (\"April\", \"April\"), \n",
    "                   (\"May\", \"May\"), (\"June\",\"June\") , (\"July\", \"July\"), (\"August\", \"Aug\"), \n",
    "                   (\"September\", \"Sep\"), (\"October\", \"Oct\"), (\"November\", \"Nov\"), (\"December\", \"Dec\")]:\n",
    "        inp = re.sub(r\"({})([0-9]+)\".format(month), r\"\\1 \\2\", inp)\n",
    "        inp = re.sub(r\"({})([0-9]+)\".format(abb), r\"\\1 \\2\", inp)\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import jsonlines\n",
    "import csv\n",
    "import re\n",
    "\"\"\"\n",
    "with open('all_html_original.csv', 'w') as fw:\n",
    "    fields = ['url', 'content']\n",
    "    writer = csv.DictWriter(fw, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "\"\"\"\n",
    "with open('WikiTableQuestions/wikidata/train.tables.jsonl') as f:\n",
    "    data = jsonlines.Reader(f)\n",
    "    for d in data:\n",
    "        f = '<table class=\"wikitable\"><tr>'\n",
    "        for h in d['header']:\n",
    "            f += \"<th> {} </th>\".format(h)\n",
    "        f += \"</tr>\"\n",
    "        for r in d['rows']:\n",
    "            f += \"<tr>\"\n",
    "            for elem in r:\n",
    "                f += \"<td> {} </td>\".format(elem)\n",
    "            f += \"</tr>\"\n",
    "        f += \"</table>\"\n",
    "        #writer.writerow({'url': d['id'], \"content\": f})\n",
    "        with open('WikiTableQuestions/wikidata/all_html/{}.html'.format(d['id']), 'w') as fw:\n",
    "            print >> fw, regex(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regex_equation(line):\n",
    "    line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)=\", r\"\\1\\2+\\3+\\4+\\5=\", line)\n",
    "    line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)-([0-9]+)=\", r\"\\1\\2+\\3+\\4=\", line)\n",
    "    line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)=\", r\"\\1\\2+\\3=\", line)\n",
    "    #line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)+([0-9]+)=\", r\"\\1\\2+\\3+\\4=\", line)\n",
    "    #line = re.sub(r\"([^+-])([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)=\", r\"\\1+\\2+\\3+\\4=\", line)    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_equation(\"67-54-69-68=270)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tmp/list.txt') as f:\n",
    "    for line in f:\n",
    "        with open('WikiTableQuestions/wikidata/all_html/' + line.strip(), 'r') as f1:\n",
    "            content = f1.readline().strip()\n",
    "        with open('WikiTableQuestions/wikidata/all_html/' + line.strip(), 'w') as f1:\n",
    "            print >> f1, regex_equation(content)\n",
    "        #print regex_equation(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('WikiTableQuestions/wikidata/train_gold.json') as f:\n",
    "    raw_output = json.load(f)\n",
    "    output = [\"/\".join([str(__) for __ in _]) for _ in raw_output]\n",
    "\n",
    "with open('WikiTableQuestions/wikidata/all_training.tsv', 'w') as fw:\n",
    "    print >> fw, \"id\\tutterance\\tcontext\\ttargetValue\"\n",
    "    with open('WikiTableQuestions/wikidata/train.jsonl') as f:\n",
    "        data = jsonlines.Reader(f)\n",
    "        idx = 1\n",
    "        for d, o in zip(data, output):\n",
    "            print >> fw, \"{}\\t{}\\t{}\\t{}\".format(idx, d['question'].replace('\\t', ''), d['table_id'], o)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "\n",
    "files = pandas.read_table('WikiTableQuestions/wikidata/all_training.tsv', delimiter=\"\\t\")\n",
    "\n",
    "output = []\n",
    "for f in os.listdir('WikiTableQuestions/wikidata/all_html/'):\n",
    "    if \"html\" in f:\n",
    "        numbering = f.split('.')[0]\n",
    "        results = files[files.context == numbering]\n",
    "        \n",
    "        for q, a in zip(results.utterance, results.targetValue):\n",
    "            tmp = {}\n",
    "            tmp[\"url\"] = 'https://raw.githubusercontent.com/wenhuchen/Interface/master/WikiTableQuestions/wikidata/all_html/{}.html'.format(numbering)\n",
    "            tmp[\"q\"] = q\n",
    "            tmp[\"a\"] = a\n",
    "            output.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WikiTableQuestions/data/all_training_new.tsv', 'w') as fw:\n",
    "    with open('WikiTableQuestions/data/all_training.tsv') as f:\n",
    "        for line in f:\n",
    "            if \"csv\" in line:\n",
    "                try:\n",
    "                    t1, t2, t3, t4 = line.strip().split('\\t')\n",
    "                except Exception:\n",
    "                    print line.strip()\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    f1, f2, f3 = t3.split('/')\n",
    "                except Exception:\n",
    "                    print t3.strip()\n",
    "                    continue\n",
    "                    \n",
    "                t3 = f2.split('-')[0] + \"-\" + f3\n",
    "                new_line = \"\\t\".join([t1, t2, t3, t4])\n",
    "                print >> fw, new_line\n",
    "            else:\n",
    "                print >> fw, line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "result = pandas.read_table('WikiTableQuestions/wikidata/all_training.tsv')\n",
    "\n",
    "new_result = result[~result.targetValue.isin(['None', 'n/a', ])]\n",
    "\n",
    "new_result.to_csv('WikiTableQuestions/wikidata/all_training_new.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for r in result['Answer.d1']:\n",
    "    print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "result = pandas.read_csv('results_v2.csv')\n",
    "filtered = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "items = []\n",
    "for i, r in filtered.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for i in range(1, num+1):\n",
    "        if r['Input.a{}'.format(i)] not in ['None', 'n/a'] and \" par \" not in r['Answer.d{}'.format(i)].lower():\n",
    "            items.append((r['Input.url{}'.format(i)], r['Input.q{}'.format(i)], r['Input.a{}'.format(i)], r['Answer.d{}'.format(i)]))\n",
    "\n",
    "with open('v2_results.json', 'w') as f:\n",
    "    json.dump(items, f, indent=2)\n",
    "    \n",
    "with open('v2_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "        \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(items) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = items[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = items[i + j][3]\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "with open('v2_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "keys = [_[1] for _ in data]\n",
    "result = pandas.read_csv('results_v2_new.csv')\n",
    "filtered = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "items = []\n",
    "for i, r in filtered.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "        \n",
    "    for i in range(1, num+1):\n",
    "        if r['Input.a{}'.format(i)] not in ['None', 'n/a'] and \" par \" not in r['Answer.d{}'.format(i)].lower() and r['Input.q{}'.format(i)] not in keys:\n",
    "            items.append((r['Input.url{}'.format(i)], r['Input.q{}'.format(i)], r['Input.a{}'.format(i)], r['Answer.d{}'.format(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "result = pandas.read_csv('results_v3.csv')\n",
    "\n",
    "filtered = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "new_items = []\n",
    "for i, r in filtered.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for i in range(1, num + 1):\n",
    "        if r['Input.a{}'.format(i)] not in ['None', 'n/a'] and \" par \" not in str(r['Answer.d{}'.format(i)]).lower():\n",
    "            new_items.append((r['Input.url{}'.format(i)], r['Input.q{}'.format(i)], r['Input.a{}'.format(i)], r['Answer.d{}'.format(i)]))\n",
    "\n",
    "new_items = new_items + items\n",
    "\n",
    "with open('v3_results.json', 'w') as f:\n",
    "    json.dump(new_items, f, indent=2)\n",
    "    \n",
    "num = 5\n",
    "with open('v3_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "        \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(new_items) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = new_items[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = new_items[i + j][3]\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print len(new_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = \"\"\n",
    "for month, abb in [(\"January\", \"Jan\"), (\"February\", \"Feb\"), (\"March\", \"March\"), (\"April\", \"April\"), \n",
    "                   (\"May\", \"May\"), (\"June\",\"June\") , (\"July\", \"July\"), (\"August\", \"Aug\"), \n",
    "                   (\"September\", \"Sep\"), (\"October\", \"Oct\"), (\"November\", \"Nov\"), (\"December\", \"Dec\")]:\n",
    "    s += \";s/({})([1-9]+)/\\\\1 \\\\2/g;s/({}),([1-9]+)//g\".format(month, abb)\n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import simplejson\n",
    "\n",
    "result = pandas.read_csv('v2_refine.csv')\n",
    "filtered_result = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "items = []\n",
    "num = 10\n",
    "for i, r in filtered_result.iterrows():\n",
    "    for i in range(1, num+1):\n",
    "        items.append((r[\"Input.url{}\".format(i)],\"-\" ,\"-\" , r[\"Answer.d{}\".format(i)]))\n",
    "\n",
    "with open('v2_rewrite.json', 'w') as f:    \n",
    "    simplejson.dump(items, f, encoding='utf-8', ignore_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import pandas\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def dealwithNum(inp):\n",
    "    inp = re.sub(r\"([^0-9.])(0)([0-9])%\", r\"\\1\\2.\\3%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\"([^0-9.])([1-9])([0-9])([0-9])%\", r\"\\1\\2\\3.\\4%\", inp)\n",
    "    inp = re.sub(r\"([^0-9.])(0)([0-9])([0-9])%\", r\"\\1\\2.\\3\\4%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\"([^0-9.])([1-9])([0-9])([0-9])([0-9])%\", r\"\\1\\2\\3.\\4\\5%\", inp)\n",
    "    inp = re.sub(r\"([^0-9.])(0)([0-9])([0-9])([0-9])%\", r\"\\1\\2.\\3\\4\\5%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\"10000%\", r\"100.00%\", inp)\n",
    "    inp = re.sub(r\"1000%\", r\"100.0%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\"([^0-9.])([1-9])([0-9])([0-9])([0-9])([0-9])%\", r\"\\1\\2\\3.\\4\\5\\6%\", inp)\n",
    "    inp = re.sub(r\"([^0-9.])(0)([0-9])([0-9])([0-9])([0-9])%\", r\"\\1\\2.\\3\\4\\5\\6%\", inp)\n",
    "    \n",
    "    inp = re.sub(r\",([0-9])%\", r\".\\1%\", inp)\n",
    "    inp = re.sub(r\",([0-9])([0-9])%\", r\".\\1\\2%\", inp)\n",
    "    inp = re.sub(r\",([0-9])([0-9])([0-9])%\", r\".\\1\\2\\3%\", inp)\n",
    "    \n",
    "    \n",
    "    #inp = re.sub(r\"([0-9]),([0-9])\", r\"\\1\\2\", inp)\n",
    "    return inp\n",
    "\n",
    "\n",
    "with open('all_html.csv', 'w') as f:\n",
    "    fields = [\"url\", \"content\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for f in os.listdir('WikiTableQuestions/wikidata/all_html/'):\n",
    "        with open('WikiTableQuestions/wikidata/all_html/' + f, 'r') as fr:\n",
    "            string = fr.readline().strip()\n",
    "        \n",
    "        writer.writerow({\"url\": f, \"content\": dealwithNum(string)})\n",
    "\n",
    "#inp_s = \"<td>123% <td> 0123% <td>012% <td>1233%  <td>12333% <td>10000% <td>01333%\"\n",
    "#dealwithNum(inp_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "table = pandas.read_csv('all_html.csv')\n",
    "\n",
    "for i, item in table.iterrows():\n",
    "    name = item['url']\n",
    "    content = item['content']\n",
    "    with open('WikiTableQuestions/wikidata/all_html/{}'.format(name), 'w') as f:\n",
    "        print >> f, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "result = pandas.read_csv('results_v4_1.csv')\n",
    "\n",
    "filtered = result[result.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "items = []\n",
    "for i, r in filtered.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for i in range(1, num+1):\n",
    "        if r['Input.a{}'.format(i)] not in ['None', 'n/a'] and \" par \" not in r['Answer.d{}'.format(i)].lower():\n",
    "            items.append((r['Input.url{}'.format(i)], r['Input.q{}'.format(i)], r['Input.a{}'.format(i)], r['Answer.d{}'.format(i)]))\n",
    "\n",
    "with open('v4_1_results.json', 'w') as f:\n",
    "    json.dump(items, f, indent=2)\n",
    "\n",
    "num = 5\n",
    "with open('v4_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "    \n",
    "    #fields.extend([\"url11\", \"s11\"])\n",
    "    \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(items) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = items[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = items[i + j][3]\n",
    "        \n",
    "        #elem['url11'] = \"https://raw.githubusercontent.com/wenhuchen/Interface/master/WikiTableQuestions/wikidata/all_html/2-1236260-1.html\"\n",
    "        #elem['s11'] = \"Total is the rank of total\"\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import csv\n",
    "import json\n",
    "import simplejson\n",
    "\n",
    "r1 = pandas.read_csv('partial_refine_v2.csv')\n",
    "r2 = pandas.read_csv('partial_refine_v3.csv')\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "results = {}\n",
    "finished = []\n",
    "for i, r in r1.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "        t = r['Answer.d{}'.format(j)]\n",
    "        if t and isinstance(t, str) and t.lower() not in [\"na\", \"none\", \"n/a\", \"no\"]:\n",
    "            if html_name not in results:\n",
    "                results[html_name] = {\"text\": [], \"label\": []}\n",
    "            results[html_name]['text'].append(t)\n",
    "            results[html_name]['label'].append(1)\n",
    "        finished.append(r['Input.s{}'.format(j)])\n",
    "        \n",
    "for i, r in r2.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "        t = r['Answer.d{}'.format(j)]\n",
    "        if t and isinstance(t, str) and t.lower() not in [\"na\", \"none\", \"n/a\", \"no\"]:\n",
    "            if html_name not in results:\n",
    "                results[html_name] = {\"text\": [], \"label\": []}        \n",
    "            results[html_name]['text'].append(r['Answer.d{}'.format(j)])\n",
    "            results[html_name]['label'].append(1)\n",
    "        finished.append(r['Input.s{}'.format(j)])\n",
    "\n",
    "r1 = pandas.read_csv('partial_neg_v2.csv')\n",
    "r2 = pandas.read_csv('partial_neg_v3.csv')\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "for i, r in r1.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        t = r['Answer.d{}'.format(j)]\n",
    "        if t and isinstance(t, str) and t.lower() not in [\"na\", \"none\", \"n/a\", \"no\"]:\n",
    "            html_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "            if html_name not in results:\n",
    "                results[html_name] = {\"text\": [], \"label\": []}\n",
    "            if r['Answer.d{}'.format(j)] not in results[html_name]['text']:\n",
    "                results[html_name]['text'].append(r['Answer.d{}'.format(j)])\n",
    "                results[html_name]['label'].append(-1)\n",
    "\n",
    "for i, r in r2.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        t = r['Answer.d{}'.format(j)]\n",
    "        if t and isinstance(t, str) and t.lower() not in [\"na\", \"none\", \"n/a\", \"no\"]:\n",
    "            html_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "            if html_name not in results:\n",
    "                results[html_name] = {\"text\": [], \"label\": []}        \n",
    "            results[html_name]['text'].append(r['Answer.d{}'.format(j)])\n",
    "            results[html_name]['label'].append(-1)\n",
    "            \n",
    "with open('READY/prelim.json', 'w') as f:\n",
    "    simplejson.dump(results, f, encoding='utf-8', ignore_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results\n",
    "inp_v2 = pandas.read_csv('v2_rewrite_input.csv')\n",
    "inp_v3 = pandas.read_csv('v3_rewrite_input.csv')\n",
    "\n",
    "not_finished = []\n",
    "done = 0\n",
    "for i, r in inp_v2.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        if r['s{}'.format(j)] not in finished:\n",
    "            not_finished.append((r['url{}'.format(j)], r['s{}'.format(j)]))\n",
    "        else:\n",
    "            done += 1\n",
    "        \n",
    "for i, r in inp_v3.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        if r['s{}'.format(j)] not in finished:\n",
    "            not_finished.append((r['url{}'.format(j)], r['s{}'.format(j)]))\n",
    "        else:\n",
    "            done += 1\n",
    "\n",
    "num = 5\n",
    "with open('v23_left_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "    \n",
    "    #fields.extend([\"url11\", \"s11\"])\n",
    "    \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(not_finished) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = not_finished[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = not_finished[i + j][1]\n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pandas.read_csv(\"input_file_yunkai.csv\")\n",
    "keys = set()\n",
    "for i, r in result.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        keys.add(r[\"url{}\".format(j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pandas.read_csv(\"v4_rewrite_input.csv\")\n",
    "#keys = set()\n",
    "for i, r in result.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        if r[\"url{}\".format(j)] in keys:\n",
    "            keys.remove(r[\"url{}\".format(j)])\n",
    "unseen_tables = list(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('READY/prelim.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "pos_length = 0\n",
    "neg_length = 0\n",
    "for k in data:\n",
    "    text = data[k]['text']\n",
    "    label = data[k]['label']\n",
    "    for t, l in zip(text, label):\n",
    "        if l == 1:\n",
    "            pos.append(t)\n",
    "            pos_length += len(t.split())\n",
    "        else:\n",
    "            neg.append(t)\n",
    "            neg_length += len(t.split())\n",
    "\n",
    "print len(pos), len(neg)\n",
    "print (pos_length + 0.0) / len(pos)\n",
    "print (neg_length + 0.0) / len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "a = Counter()\n",
    "a.update([1,2,3])\n",
    "a.update([1,3,4])\n",
    "\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import csv\n",
    "\n",
    "r1 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3580679_batch_results.csv')\n",
    "r2 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3584639_batch_results.csv')\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "r3 = pandas.concat([r1, r2])\n",
    "finished = []\n",
    "\n",
    "r1_inp = pandas.read_csv('v23_left_rewrite_input.csv')\n",
    "r2_inp = pandas.read_csv('v4_rewrite_input.csv')\n",
    "r3_inp = pandas.concat([r1_inp, r2_inp])\n",
    "\n",
    "for i, r in r3.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        #if r[\"url{}\".format(j)] in keys:\n",
    "        finished.append(r[\"Input.s{}\".format(j)])\n",
    "\n",
    "print len(finished)\n",
    "unfinished = []\n",
    "done = 0\n",
    "finished_old = []\n",
    "for i,r in r3_inp.iterrows():\n",
    "    if \"url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        #if r[\"url{}\".format(j)] in keys:\n",
    "        if r[\"s{}\".format(j)]  not in finished:\n",
    "            unfinished.append((r[\"url{}\".format(j)], r[\"s{}\".format(j)]))\n",
    "        else:\n",
    "            finished_old.append(r[\"s{}\".format(j)])\n",
    "            done += 1\n",
    "\n",
    "print done\n",
    "with open('v234_rest_rewrite_input.csv', 'w') as f:\n",
    "    #json.dump(items, f, indent=2)\n",
    "    num = 5\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "        \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(unfinished) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = unfinished[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = unfinished[i + j][1]\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r1 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3583922_batch_results.csv')\n",
    "r2 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3579309_batch_results.csv')\n",
    "r3 = pandas.read_csv('/Users/wenhuchen/Downloads/Batch_3579785_batch_results.csv')\n",
    "\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "r3 = r3[r3.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "r4 = pandas.concat([r1, r2, r3])\n",
    "finished = []\n",
    "\n",
    "\n",
    "for i, r in r4.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        finished.append(r[\"Input.s{}\".format(j)])\n",
    "print len(finished)\n",
    "\n",
    "r1_inp = pandas.read_csv('v2_rewrite_input.csv')\n",
    "r2_inp = pandas.read_csv('v3_rewrite_input.csv')\n",
    "r3_inp = pandas.read_csv('v4_rewrite_input.csv')\n",
    "r4_inp = pandas.concat([r1_inp, r2_inp, r3_inp])\n",
    "unfinished = []\n",
    "done = 0\n",
    "for i,r in r4_inp.iterrows():\n",
    "    if \"url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        #if r[\"url{}\".format(j)] in keys:\n",
    "        if r[\"s{}\".format(j)]  not in finished:\n",
    "            unfinished.append((r[\"url{}\".format(j)], r[\"s{}\".format(j)]))\n",
    "        else:\n",
    "            done += 1\n",
    "\n",
    "print done\n",
    "with open('v234_rest_rewrite_fake_input.csv', 'w') as f:\n",
    "    num = 5\n",
    "    fields = []\n",
    "    for j in range(1, num + 1):\n",
    "        fields.append(\"url{}\".format(j))\n",
    "        fields.append(\"s{}\".format(j))\n",
    "        \n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for i in range(0, len(unfinished) - num, num):\n",
    "        elem = {}\n",
    "        for j in range(num):\n",
    "            elem[\"url{}\".format(j + 1)] = unfinished[i + j][0]\n",
    "            elem[\"s{}\".format(j + 1)] = unfinished[i + j][1]\n",
    "            \n",
    "        writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def substitute(string):\n",
    "    string = string.lower()\n",
    "    \n",
    "    string = string.replace(r\"\\n\", \"\")\n",
    "    string = string.replace(r\"\\\\\", \"\")\n",
    "    string = string.replace(r\"!$\", \"\")\n",
    "    string = string.replace(r\"#\", \"\")\n",
    "    string = string.replace('–', '-')\n",
    "    string = string.replace('−', '-')\n",
    "    string = string.replace('−', '-')\n",
    "    string = string.replace('â', '')\n",
    "    string = string.replace('“', '')\n",
    "    string = string.replace('€', '')    \n",
    "    string = string.replace(\"√\", \"\")\n",
    "    string = string.replace(r'\"', '')\n",
    "    string = string.replace(r'\\.0 ', '')\n",
    "    string = string.replace(r'¹', '')\n",
    "    string = string.replace(r'‡', '')\n",
    "    string = string.replace(r'«', '')\n",
    "    string = string.replace(r'»', '')\n",
    "    \n",
    "    string = string.replace(r\"'s\", \" 's\")\n",
    "    string = string.replace(r\"'re\", \" 're\")\n",
    "    string = string.replace(r\"'nt\", \" not\")\n",
    "    string = string.replace(r'@', ' ')\n",
    "    string = string.replace(r'`', '')\n",
    "    string = string.replace(r'•', ' ')\n",
    "    string = string.replace(r'·', ' ')\n",
    "    string = string.replace(r'²', ' square')\n",
    "    string = string.replace(r'\\[', '')\n",
    "    string = string.replace(r'\\]', '')\n",
    "    string = string.replace(r'\\{', '')\n",
    "    string = string.replace(r'\\}', '')\n",
    "    string = string.replace(r'\\|', '')\n",
    "    string = string.replace(r'\\^', '')\n",
    "    string = string.replace(r'”', '')\n",
    "\n",
    "    string = re.sub(r'\\[.+\\]', ' ', string)\n",
    "    string = re.sub(r',([0-9]{3})(?![0-9])', r'\\1', string)\n",
    "    #string = re.sub(r'([0-9]{1-3}),([0-9]{3}),([0-9]{3})([^0-9])', r\"\\1\\2\\3\\4\", string)\n",
    "    #string = re.sub(r'([0-9]{1-3}),([0-9]{3}),([0-9]{3}),([0-9]{3})([^0-9])', r\"\\1\\2\\3\\4\\5\", string)\n",
    "    #string = re.sub(r'([0-9]{1-3}),([0-9]{3}),([0-9]{3}),([0-9]{3}),([0-9]{3})([^0-9])', r\"\\1\\2\\3\\4\\5\\6\", string)\n",
    "\n",
    "    string = re.sub(r'([^,]),', r\"\\1 ,\", string)\n",
    "    string = re.sub(r',([^,])', r\", \\1\", string)\n",
    "    string = re.sub(r'([^(])\\(', r\"\\1 (\", string)\n",
    "    string = re.sub(r'\\)([^)])', r\") \\1\", string)\n",
    "    string = re.sub(r'\\.\\)', r\")\", string)\n",
    "    string = re.sub(r'([0-9]+)-([0-9]+)-([0-9]+)-([0-9]+)=', r\"\\1+\\2+\\3+\\4=\", string)\n",
    "    string = re.sub(r'([0-9]+)-([0-9]+)-([0-9]+)=', r\"\\1+\\2+\\3=\", string)\n",
    "    string = re.sub(r'([0-9]+)-([0-9]+)=', r\"\\1+\\2=\", string)\n",
    "    string = re.sub(r\"' +([0-9])\", r\"'\\1\", string)\n",
    "\n",
    "    string = re.sub(r\"([^ ])\\+\", r\"\\1 +\", string)\n",
    "    string = re.sub(r\"\\+([^ ])\", r\"+ \\1\", string)\n",
    "    string = re.sub(r\"([^ ])=\", r\"\\1 =\", string)\n",
    "    string = re.sub(r\"=([^ ])\", r\"= \\1\", string)\n",
    "\n",
    "    string = re.sub(r\"-([^- ])\", r\"- \\1\", string)\n",
    "    string = re.sub(r\"([^- ])-\", r\"\\1 -\", string)\n",
    "    string = re.sub(r\"-([^- ])\", r\"- \\1\", string)\n",
    "    string = re.sub(r\"/([^ ])\", r\"/ \\1\", string)\n",
    "    string = re.sub(r\"([^ ])/\", r\"\\1 /\", string)\n",
    "    string = string.replace(r'()', '')\n",
    "    \n",
    "    string = re.sub(r'([^0-9 ]):', r\"\\1 :\", string)\n",
    "    string = re.sub(r':([^0-9 ])', r\": \\1\", string)\n",
    "    string = re.sub(r'([0-9])pm', r\"\\1 pm\", string)\n",
    "    string = re.sub(r'([0-9])am', r\"\\1 am\", string)\n",
    "    string = re.sub(r'([0-9])rpm', r\"\\1 rpm\", string)\n",
    "    string = re.sub(r'([0-9])km', r\"\\1 km\", string)\n",
    "    string = re.sub(r'([0-9])cm', r\"\\1 cm\", string)\n",
    "    string = re.sub(r'([0-9])m', r\"\\1 m\", string)\n",
    "    string = re.sub(r'([0-9])mm', r\"\\1 mm\", string)\n",
    "    string = re.sub(r'([0-9])kg', r\"\\1 kg\", string)\n",
    "    string = re.sub(r'([0-9])g', r\"\\1 g\", string)\n",
    "    string = re.sub(r'([0-9])kw', r\"\\1 kw\", string)\n",
    "    string = re.sub(r'([0-9])kv', r\"\\1 kv\", string)\n",
    "    string = re.sub(r'([0-9])mph', r\"\\1 mph\", string)\n",
    "    #string = re.sub(r'([0-9])@', r\"\\1 @\", string)\n",
    "    #string = re.sub(r'@([0-9])', r\"@ \\1\", string)\n",
    "    string = re.sub(r'category : articles with hcards', r\"\", string)\n",
    "    string = re.sub(r'category : articles with hcard', r\"\", string)\n",
    "    string = re.sub(r'category : articles without hcard', r\"\", string)\n",
    "    \n",
    "    string = re.sub(r\"\\.+$\", '', string)\n",
    "    string = re.sub(r',+$', '', string)    \n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    string = re.sub(r'\\.+','.',string)\n",
    "    string = re.sub(r',+',',',string)\n",
    "    string = re.sub(r'^ ', '', string)\n",
    "    string = re.sub(r' $', '', string)\n",
    "    string = re.sub('70 - 76 - 68 - 214', '70 + 76 + 68 = 214', string)\n",
    "    return string\n",
    "\n",
    "#print substitute(\"fa\\g##ga,/// she got(ff)ff...\")\n",
    "#print substitute('(5) fa:\"12-12-13=31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blacklist(string):\n",
    "    black = ['bye', \"no chart for\", \"km (mi)\", \"lb·ft\", \"ft (m)\", \"kg (lb)\", \n",
    "             \"a report of report\", \"report was report\", \"is 'report'\", \"?\"]\n",
    "    for b in black:\n",
    "        if b in string:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "with open('data/short_subset.txt') as f:\n",
    "    limit_length = [_.strip() for _ in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "import pandas\n",
    "\n",
    "r1_1 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_0.csv\")\n",
    "r1_2 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_1.csv\")\n",
    "\n",
    "r1 = pandas.concat([r1_1, r1_2])\n",
    "r1 = r1[r1.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "f = open('/tmp/output.txt', 'w')\n",
    "\n",
    "index = 0\n",
    "finished = {}\n",
    "trash = []\n",
    "for i,r in r1.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1] + '.csv'\n",
    "        if html_name not in limit_length:\n",
    "            continue\n",
    "        if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "            #trash.append(html_name)\n",
    "            print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3 and not blacklist(orig_input):\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            if 'hcard' not in replaced_sent:\n",
    "                print >> f, replaced_sent\n",
    "                index += 1\n",
    "                if html_name not in finished:\n",
    "                    finished[html_name] = [[replaced_sent], [1]]\n",
    "                else:\n",
    "                    finished[html_name][0].append(replaced_sent)\n",
    "                    finished[html_name][1].append(1)\n",
    "                    \n",
    "r2_1 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_2.csv\")\n",
    "r2_2 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_3.csv\")\n",
    "r2_3 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/refine_4.csv\")\n",
    "\n",
    "r2 = pandas.concat([r2_1, r2_2, r2_3])\n",
    "r2 = r2[r2.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "for i,r in r2.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1] + '.csv'\n",
    "        if html_name not in limit_length:\n",
    "            continue     \n",
    "        if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "            print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3 and not blacklist(orig_input):\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            if 'hcard' not in replaced_sent:\n",
    "                print >> f, replaced_sent\n",
    "                index += 1\n",
    "                if html_name not in finished:\n",
    "                    finished[html_name] = [[replaced_sent], [1]]\n",
    "                else:\n",
    "                    finished[html_name][0].append(replaced_sent)\n",
    "                    finished[html_name][1].append(1)\n",
    "\n",
    "\n",
    "r3_1 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/fake_0.csv\")\n",
    "r3_2 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/fake_1.csv\")\n",
    "r3_3 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/fake_2.csv\")\n",
    "r3_4 = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest/fake_3.csv\")\n",
    "\n",
    "r3 = pandas.concat([r3_1, r3_2, r3_3, r3_4])\n",
    "r3 = r3[r3.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "f = open('/tmp/output.txt', 'w')\n",
    "\n",
    "index = 0\n",
    "for i,r in r3.iterrows():\n",
    "    if \"Input.url10\" in r:\n",
    "        num = 10\n",
    "    else:\n",
    "        num = 5\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        html_name = r['Input.url{}'.format(j)].split('/')[-1] + '.csv'\n",
    "        if html_name not in limit_length:\n",
    "            continue\n",
    "        if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "            print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3 and not blacklist(orig_input):\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            if 'hcard' not in replaced_sent:\n",
    "                print >> f, replaced_sent\n",
    "                index += 1\n",
    "                if html_name not in finished:\n",
    "                    finished[html_name] = [[replaced_sent], [0]]\n",
    "                else:\n",
    "                    finished[html_name][0].append(replaced_sent)\n",
    "                    finished[html_name][1].append(0)\n",
    "\n",
    "f.close()\n",
    "print index\n",
    "import json\n",
    "with open(\"READY/training_all.json\", 'w') as f:\n",
    "    json.dump(finished, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print trash\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print trash\n",
    "for l in trash + [\"2-11916083-12.html\", \"2-1236260-1.html\"]:\n",
    "    try:\n",
    "        shutil.move(\"data/all_csv/\" + l + \".csv\", \"data/trash_csv/\" + l + \".csv\")\n",
    "    except Exception:\n",
    "        print \"error, {}\".format(\"data/all_csv/\" + l + \".csv\", \"data/trash_csv/\" + l + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import csv\n",
    "\n",
    "url = open('WikiTableQuestions/wikidata/all_html/2-18178551-5.html').readline().strip()\n",
    "soup = BeautifulSoup(url, \"html.parser\")\n",
    "\n",
    "table = soup.findAll(\"table\", {\"class\":\"wikitable\"})[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "with open('/tmp/editors.csv', \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        csv_row = []\n",
    "        for cell in row.findAll([\"td\", \"th\"]):\n",
    "            csv_row.append(cell.get_text())\n",
    "        writer.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "for filename in os.listdir('WikiTableQuestions/wikidata/all_html/'):\n",
    "    if filename.endswith(\".html\"):\n",
    "        url = open('WikiTableQuestions/wikidata/all_html/' + filename).readline().strip()\n",
    "        soup = BeautifulSoup(url, \"html.parser\")\n",
    "        table = soup.findAll(\"table\", {\"class\":\"wikitable\"})\n",
    "        if len(table) > 0:\n",
    "            table = table[0]\n",
    "            rows = table.findAll(\"tr\")\n",
    "            with open('WikiTableQuestions/wikidata/all_csv/' + filename + '.csv', 'w') as csvfile:\n",
    "                spamwriter = csv.writer(csvfile, delimiter='#')\n",
    "                for row in rows:\n",
    "                    csv_row = []\n",
    "                    for cell in row.findAll([\"td\", \"th\"]):\n",
    "                        print cell.get_text()\n",
    "                        csv_row.append(substitute(cell.get_text()))\n",
    "                        print substitute(cell.get_text())\n",
    "                        print \"\"\n",
    "                    spamwriter.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import csv\n",
    "import urllib\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open('READY/training_all.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "files = data.keys()\n",
    "\n",
    "unseen = []\n",
    "with open('data/short_subset.txt') as fs:\n",
    "    for f in fs:\n",
    "        f = f.strip()\n",
    "        if f not in files and f in tiny_mapping:\n",
    "            unseen.append(f)\n",
    "with open(\"data/v5_unseen.json\", 'w') as f:\n",
    "    json.dump(unseen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import json\n",
    "import urllib\n",
    "import json\n",
    "import os \n",
    "import csv\n",
    "\n",
    "with open('data/table_to_page_new.json', 'r') as f:\n",
    "    tiny_mapping = json.load(f)\n",
    "    \n",
    "with open(\"data/v5_unseen.json\", 'r') as f:\n",
    "    unseen = json.load(f)\n",
    "    \n",
    "with open('v5_write_input.csv', 'w') as f:\n",
    "    fields = [\"url1\", \"wiki1\", \"topic1\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for k in unseen:\n",
    "        if k in tiny_mapping:\n",
    "            writer.writerow({\"url1\": 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k, \n",
    "                            \"wiki1\": tiny_mapping[k][1], \"topic1\": tiny_mapping[k][0]})\n",
    "        else:\n",
    "            writer.writerow({\"url1\": 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k, \"wiki1\": \"javascript:void(0)\", \"topic1\": \"None\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tmp/table_to_page.json', 'r') as f:\n",
    "    old_tiny_mapping1 = json.load(f)\n",
    "with open('data/table_to_page.json', 'r') as f:\n",
    "    old_tiny_mapping2 = json.load(f)\n",
    "    \n",
    "new_tiny_mapping = {}\n",
    "for k, v in old_tiny_mapping2.iteritems():\n",
    "    new_tiny_mapping[k] = [substitute(old_tiny_mapping1[k].split('/')[-1].replace('_', ' ')), old_tiny_mapping2[k]]\n",
    "\n",
    "with open('data/table_to_page_new.json', 'w') as f:\n",
    "    json.dump(new_tiny_mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tmp/g.txt', 'w') as f:\n",
    "    x = \"https://en.wikipedia.org/wiki/2007%E2%80%9308_Scottish_Second_Division\"\n",
    "    print >> f, [\"topic\", urllib.unquote(x).split('/')[-1]]\n",
    "    x = 'https://en.wikipedia.org/wiki/Ana_Timoti%C4%87'\n",
    "    print >> f, {\"topic\": urllib.unquote(x).split('/')[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "name = files[0]\n",
    "tab = pandas.read_csv('data/all_csv/' + name + '.csv', delimiter='#')\n",
    "sent = data[name][0][0]\n",
    "label = data[name][1][0]\n",
    "\n",
    "tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "activations = []\n",
    "for i, t in enumerate(tab.columns):\n",
    "    for j, v in enumerate(tab[t]):\n",
    "        if isinstance(v, str) and len(v.split(' ')) > 2:\n",
    "            if fuzz.partial_ratio(v, sent) > 95:\n",
    "                print t, j\n",
    "        else:\n",
    "            if str(v) in sent:\n",
    "                print t, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa = 0.505406867561\n",
      "Counter({'Entailed': 762, 'Contradictory': 650, 'Problematic': 63, 'Neutral': 40})\n",
      "('2-14347546-5.html.csv', 'the record for the december under 28 with a game under 26 with less than 38 points and a score of 0 - 1 in ot is 12 - 6 - 6 - 1')\n",
      "['Contradictory', 'Entailed', 'Neutral']\n",
      "('2-17162128-3.html.csv', 'the united states is the country when to par is + 11 when the total is more than 151')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-21781578-2.html.csv', 'the original air date for season 2 was january 13 , 2002')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-16387653-1.html.csv', 'during the st. kilda and fitzroy game the crowd size was 12656')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-14319023-2.html.csv', 'in 2009 , the mixed double consists of didit juang indrianto and yayu rahayu and hermansyah is the boys singles')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-176524-1.html.csv', 'drummond village is 8.91 in area')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-12259758-6.html.csv', 'the score in the final on june 22 , 2013 on clay was 4 - 6 , 4 - 6')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-12536586-1.html.csv', 'the week 14 game against the buffalo bills reported the second highest attendance of the season')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1204998-2.html.csv', 'w\\xc7\\x94p\\xc3\\xadng xi\\xc3\\xa0n is the pinyin for the traditional chinese name \\xe6\\xad\\xa6\\xe5\\xb9\\xb3\\xe7\\xb8\\xa3')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-12207449-6.html.csv', 'white sox recorded 63 - 56 on august 12')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-10953197-4.html.csv', 'dee johnson was the writer for the episode with production codes 2395096 & 2395877')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1028356-3.html.csv', 'the score of the match was 6 - 1 , 7 - 6 (3) with the opponents of anne hobbs and andrew castle')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11611293-10.html.csv', 'jp maher had a date of 19 - 02 - 2003')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-23696862-6.html.csv', 'andras koroknai had 2 wsop cashes and was 4rth in final place')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1204998-2.html.csv', 'shanghang county has an area of 3099')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-14269540-5.html.csv', 'it was before may 21 when snell (2 - 3) received the loss for the game and has a 22 - 24 record')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1028356-3.html.csv', 'the wimbledon (2) championship was played in 1988')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11548185-5.html.csv', 'the lowest episode number with an air date of october 31 2001 , is episode 83')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-2849652-2.html.csv', 'in 1978 there was 1 winter season with exactly 1 division')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11872185-5.html.csv', 'the studio red chillies entertainment has two films in the top ten list')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-10953197-4.html.csv', 'one is the total number of episodes with the production code 2395120')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-16965329-5.html.csv', 'the guayaquil tournament was played on 5 november 2007 with a score of 3 - 6 , 7 - 6 (6) , 4 - 7')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('1-13619053-9.html.csv', 'indiana was the team in a game with a record less than 15 - 63')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-26593762-3.html.csv', 'the table position for the team whose outgoing manager was andy thorn was 9th')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17162128-3.html.csv', 'hale irwin is the only player from the united states to win multiple years')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11780179-1.html.csv', 'kazakhstan is the country which gdp (nominal) of $6.4 billion and has a population greater than 5550239')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11780179-1.html.csv', 'turkmenistan has a population is less than 5550239 and gdp (nominal) greater than $6.4 billion')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-176524-1.html.csv', '18.06 is the area in the census of the community ranked 636 of 5008')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-14344187-13.html.csv', 'matt brait was later than the 11th round')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-1478772-2.html.csv', '77:54 minutes was the length of the game that was played with the new york jets at home')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17301013-5.html.csv', 'when st. louis is the visitor , the record is between 12 - 28 - 5 and 16 - 24 - 5')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-14344187-13.html.csv', 'greg johnson is from sweden')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-2509350-3.html.csv', 'the quechua language in el villar is the highest in the municipality')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-11870943-8.html.csv', 'the us open had a 2r categorization between 2010 and 2012 and an sf categorization between 2007 and 2009')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-15532127-1.html.csv', 'the scores were 1 - 0 , 3 - 0 , and 4 - 0 in competitions with a result of 5 - 0 at the 1996 afc asian cup qualification')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-12552861-1.html.csv', \"tan joe hok won the men 's singles and robert b. williams ethel marshall won the mixed doubles in 1957\")\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-10953197-4.html.csv', 'kimberly costello is the writer for the episodes with a production codes 2395114 and 2395115')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-14319023-2.html.csv', 'in 2010 , when mixed doubles is danny bawa chrisnanta and debby susanto , the boys singles is nugroho andi saputro')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-26593762-3.html.csv', \"brial laws' team acquired a position higher than 9th\")\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-10287593-1.html.csv', '5 is the average number of fixtures when 2 clubs are involved')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-15532127-1.html.csv', 'the 1996 afc asian cup qualification is the competition with a score of 4 - 0 and result of 5 - 0 on june 22nd , 1997')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-28081876-4.html.csv', 'season 6 , series number 11 was directed by rob schrab and was writteh by one person')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-16387653-1.html.csv', 'when footscray played richmond at home they scored 75')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-1478772-2.html.csv', 'the oakland raiders were the home team for the game played on december 24 , 1977')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-10883333-13.html.csv', 'footscray was the away team when north melbourne was the home team')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-10696381-1.html.csv', 'dave nugent played defensive tackle')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-10953197-4.html.csv', 'kimberly costello wrote all the episodes till production code 2395114')\n",
      "['Contradictory', 'Contradictory', 'Neutral']\n",
      "('2-1717109-6.html.csv', 'in 1983 , 87 , and 88 tournament results followed a / 1r / a')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('1-261927-1.html.csv', 'the location for the colors green & black is less than 10235')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-16387653-1.html.csv', 'when playing against carlton , collingwood was the away team')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-2509350-3.html.csv', 'the language for villa 1167 is spanish')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-16013858-11.html.csv', 'set 2 was 25 - 20 when set 3 was 22 - 25')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1717109-6.html.csv', 'in 1989 the result for the tournament at indian wells was qf')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12805568-1.html.csv', 'construction started on 01.06.1964 for the vver - 365 (prototype) with a net capacity of 197 mw and was shutdown on 29.08.1990')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-17358229-1.html.csv', 'european race walking cup has a higher position than 2nd in the competition')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-23696862-6.html.csv', 'the person who had $39371 wsop earnings had three wsop cashes')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-27437601-2.html.csv', 'may 5 , 2011 is the 1st air date for season 12')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12259758-6.html.csv', 'micha\\xc3\\xabl llodra nenad zimonji\\xc4\\x87 was the partner for outcome of runner - up and date of august 7 , 2011')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-14347546-5.html.csv', 'there were 38.0 points in the game over 25 whose opponent was the buffalo sabres')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-29728787-1.html.csv', 'wolfgang sawallische was the count when dietrich fischer dieskau was the conductor')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1122578-1.html.csv', 'the grid for denny hulme is over 6.0')\n",
      "['Contradictory', 'Contradictory', 'Neutral']\n",
      "('2-10883333-13.html.csv', 'fitzroy was the home team when the away team was south melbourne')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1234504-1.html.csv', 'wheel arrangement 4 - 4 - 0 , has baldwin as builder')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-16387653-1.html.csv', 'carlton beat collingwood by a score of 109 - 82')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-14986292-1.html.csv', 'the population density for duque de caxias in 2010 was more than 1840')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-16013858-11.html.csv', 'on june 26 set 1 was 21 - 19 when set 3 was 26 - 24')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-14344187-13.html.csv', 'in round 3 , the player in the center position was reid simpson')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17301013-5.html.csv', 'record st. louis the visitor is 26 - 5 - 14')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12259758-6.html.csv', 'the score was 4 - 6 , 4 - 6 in the final for the event right after january 12 , 2013')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('1-29728787-1.html.csv', 'karl bohm is the count when hermann uhde was the conductor')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-12036377-1.html.csv', 'napoli - roma first match of the serie a was on 30 january 1938')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-26593762-3.html.csv', 'dougie freedman was the incoming manager for the team that darren ferguson served as outgoing manager for')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1478772-2.html.csv', 'the game against the baltimore colts was 82:40 long')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-14986292-1.html.csv', 'japeri had a population more than 95391')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-16678300-2.html.csv', 'week 10 took place on november 6 , 1977')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17162128-3.html.csv', 'steve jones is the only player from the united states and the only play with a total of 154')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-21781578-2.html.csv', 'the season number with the highest u.s. viewers (millions) was season no. 13 with 13.00')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('1-10953197-4.html.csv', 'episode melrose unguled is numbered 124.0')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12036377-1.html.csv', 'the result of the roma - napoli match was 1 - 1 / 3 - 0 / 1 - 0 / 1 - 2 / 1 - 2 / 3 - 2 / 1 - 2 / 0 - 0 / 2 - 2 / 1 - 0 / 0 - 1 / 2 - 1 / 1 - 1 / 2 - 1 / 0 - 3 / 1 - 2 / 0 - 0 / 0 - 0 / 1 - 0 / 2 - 0 / 1 - 1 / 1 - 2 / 0 - 0 / 3 - 0 / 1 - 0 / 3 - 2 / 3 - 3 / 1 - 0 / 2 - 0 / 2 - 0 / 0 - 0 / 0 - 0 / 1 - 2')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-13619053-9.html.csv', 'milwaukee was the team that played after april 1')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-11753791-1.html.csv', 'a team that has a more wins than losses has 0 ties')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-10883333-13.html.csv', 'essendon was the away team when collingwood played as the home team')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-1341640-14.html.csv', 'paul findley , district illinois 20 , was first elected after henry hyde , district illinois 6')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-11780179-1.html.csv', 'the country that has a gdp per capita (nominal) of $29.9 billion is tajikistan')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12552861-1.html.csv', \"tan joe hok won the men 's singles in 1958\")\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1204998-2.html.csv', 'when hakka is zh\\xc4\\x81ngp\\xc3\\xadng sh\\xc3\\xac the area is 2879')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-23696862-6.html.csv', 'jesse sylvia had 0 wsop earnings')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-10696381-1.html.csv', 'dave stachelski was picked before the 5th round')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1076869-2.html.csv', 'the bb cw for 1963 is 22 / and the riaa is g')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-18223552-1.html.csv', 'the match 31 july 1993 with result of 0 - 1 against kaizer chiefs had 65000 people')\n",
      "['Contradictory', 'Neutral', 'Contradictory']\n",
      "('1-261927-1.html.csv', 'more than one institution have the colors blue and gold')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-1341640-14.html.csv', 'in 1974 republican phil crane beat henry hyde who was a democrat')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-26211058-1.html.csv', 'when the specimen weight / size is 1000 g / 8.79 cm , the estimated exposure (mrem) / hr* is 0.00')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-27821519-1.html.csv', 'toyota is the manufacturer of no. 50')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-20626467-1.html.csv', 'the jockey for group 1 at the 1400 m distance at randwick was hugh bowman , with a result of 1st')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-12206211-4.html.csv', 'the score in the date of june 7 was 12 - 3')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-17327458-1.html.csv', 'more than two vacancies happened on may 25')\n",
      "['Contradictory', 'Neutral', 'Contradictory']\n",
      "('2-1234504-1.html.csv', 'mason built the order on november 1881 of more than 54 tc&stl no. (1883 - 84)')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-21781578-2.html.csv', 'john david coles had 5 episodes that were written by him and that were also directed by him')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-23696862-6.html.csv', 'the person who had 0 wsop earnings had five wsop cashes')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17301013-5.html.csv', 'the home team on january 2 , which had 23 points , is buffalo')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1028356-3.html.csv', 'with jim pugh as partner , the score of the match was 7 - 5 , 6 - 2')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-17327458-1.html.csv', 'mixu paatelainen was suspended on may 29th')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-261927-1.html.csv', 'the colors of massachusetts institute of technology (mit) are green and black')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-24055352-1.html.csv', \"d'andre bell is the last player from los angeles\")\n",
      "['Contradictory', 'Neutral', 'Neutral']\n",
      "('2-17162128-3.html.csv', 'steve jones was the player when the total is more than 154')\n",
      "['Contradictory', 'Contradictory', 'Neutral']\n",
      "('2-1234504-1.html.csv', 'porter built the order on november 1881 of less than 54 tc&stl')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12552861-1.html.csv', \"when erland kops won the men 's singles in 1958 , jean miller won the women 's singles\")\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-14319023-2.html.csv', 'in 2010 , the girls doubles were ayu pratiwi and anggi widia and the boys doubles were jones ralfy jansen and dandi prabudita')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-14344187-13.html.csv', 'matt brait was below 11')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-12206211-4.html.csv', 'the game of june 3 , was against the expos')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-182499-1.html.csv', 'enzyme ala synthase has a location mitochondrion and porphyria of porphyria cutanea tarda')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1204998-2.html.csv', 'shanghang xian has a population that is below 350000')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-10790397-14.html.csv', 'the team that scored 9.8 (62) is richmond')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17162128-3.html.csv', 'the united states was the country with the player hale irwin not in the table')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1717109-6.html.csv', 'in 1987 and 1989 the tournament results where the same 2r')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-16013858-11.html.csv', \"when set 4 was 19 - 25 , set 2 was 21 - 25 , and there wasn't a set 5\")\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-26593762-3.html.csv', 'george burley was the outgoing manager of the team whose incoming manager was aidy boothroyd')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-24055352-1.html.csv', \"d'andre bell 's height is 6 - 6 the same lance storrs\")\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-13598796-1.html.csv', 'jan johansen is ranked in last place')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-10953197-4.html.csv', 'frank south directed episode 129 titled full metal betsy')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-10696381-1.html.csv', 'purdue was the college with an overall pick of smaller than 187 and a round larger than 7 for the defensive end position')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11780179-1.html.csv', '$5330 is the gdp per capita (nominal) with population less than 5125693')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-14344187-13.html.csv', 'canada is in left wing position in round 2')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-261927-1.html.csv', 'salve regina university is the institution with the nickname bison')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-17327458-1.html.csv', 'derek ferguson left a position before january 24')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-182499-1.html.csv', 'the location of substrate porphobilinogen and chromosome 11q23.3 is mitochondrion')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-2509350-3.html.csv', 'the language for sopachuy municipality 6261 is guaran\\xc3\\xad')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-20626467-1.html.csv', 'the time for entrant weighing 54.4 kg was 1 - 38.28 in wawrick farm')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12531523-5.html.csv', 'ian poulter from the united states had a score of 214')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12259758-6.html.csv', 'max mirnyi was the partner for the win on june 23 , 2012 against juan sebasti\\xc3\\xa1n cabal dmitry tursunov')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('1-1007688-1.html.csv', 'an undetermined number of cases of relapsing fever happened during 3000 cases of malaria')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-16387653-1.html.csv', 'footscray scored 75 against the visiting richmond team')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-1165048-1.html.csv', '8 - 11 was the win / loss of coach peter german')\n",
      "['Contradictory', 'Contradictory', 'Neutral']\n",
      "('2-11963536-7.html.csv', 'on february 22nd , 2008 the rockets were visiting the spurs')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('1-1007688-1.html.csv', 'when typhoid fever had the highest cases with 424 , smallpox had the lowest cases at 8')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-182499-1.html.csv', 'the substrate uroporphyrinogen iii and the product protoporphyrinogen ix , is on chromosome 10q25.2 - q26.3')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-184391-1.html.csv', 'the state of estonia has less than 19000 in the nominal gdp world bank , 2009 (million usd)')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-16013858-11.html.csv', 'the information is not na when matching dates jun 25 and when set 3 is 19 - 25')\n",
      "['Contradictory', 'Neutral', 'Entailed']\n",
      "('2-13598796-1.html.csv', 'there are 2 draws with more that 107 points by the artist ellinor franzen')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-14344187-13.html.csv', 'in round 2 sweden is in the right wing position')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-26211058-1.html.csv', 'the specimen weight / size for the estimated exposure (mrem) / hr* is 0.03 is 1')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11872185-5.html.csv', 'dabangg is the movie with an opening day on wednesday and a rank of 10')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12536586-1.html.csv', 'the game on october 9 , 1983 had the lowest attendance of the year')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-16785772-6.html.csv', 'the lowest number of points scored when there were 1500 in attendance in boston was 9')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-1478772-2.html.csv', 'december 24 , 1977 was the date the st. louis rams played with the baltimore colts at home')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-24055352-1.html.csv', \"kammeon holsey 's height is 6 - 8 the same lance storrs\")\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1234504-1.html.csv', 'there are only two constructions (610 & 659) that have the wheel arrangement of 2 - 6 - 0')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-17358229-1.html.csv', 'munich , germany hosted 12th position european championships')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-28081876-4.html.csv', 'the episode with production code 211 was i see her face everywhere')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11870943-8.html.csv', 'hte categorization in 2011 was a when it was a in 2008 , a in 2012 and qf in 2010')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-27437601-2.html.csv', 'the series number with the episode called shock waves is 250.0')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1122578-1.html.csv', 'there are laps for a time / retired of tyre and a grid smaller than 10')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-1007688-1.html.csv', 'there were 170 cases of typhoid fever and 4 cases of relapsing fever in 1929')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-261927-1.html.csv', 'massachusetts institute of technology (mit) is the location with the nickname blue & gold')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11786754-5.html.csv', \"rob turner has the highest avg. at 11.7 and the third most td 's\")\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-17301013-5.html.csv', 'the home team on january 30 with 23 points as pittsburgh')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12207449-6.html.csv', 'the final score of the game played on august 6 is 8 - 3')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17162128-3.html.csv', 'the total for hale irwin is listed as wd in year (s) won in 1996')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-2849652-2.html.csv', 'the 1st tournament associated with bowling was in 2006')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17162128-3.html.csv', 'steve jones is the player when the country is united states and the total is less than 154')\n",
      "['Contradictory', 'Contradictory', 'Neutral']\n",
      "('2-11872185-5.html.csv', 'the average rank of dharma productions is four')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12206211-4.html.csv', 'at the record 32 - 37 the royals lost by 4')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12531523-5.html.csv', 'phil mickelson has the score 72 + 67 + 69 = 208')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-10953197-4.html.csv', 'no episode has a production code of 2395113a')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-18223552-1.html.csv', '11 , 372 people attended the match on 3 august 1993 with result f - a 0 - 2 and h / a of n')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-1717109-6.html.csv', 'the 1987 result for the canada tournament was rr')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-14319023-2.html.csv', 'when the girls singles is lindaweni fanetri , the mixed doubles is wifqi windarto debby susanto after the year 2007')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-27437601-2.html.csv', 'there are more than 241 series for season 12')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1007688-1.html.csv', 'the number of typhoid fever cases in 1934 was approximately the same in 1929 and 1930')\n",
      "['Contradictory', 'Neutral', 'Neutral']\n",
      "('1-21781578-2.html.csv', 'the season number for production code e2110 was 14')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-16965329-5.html.csv', 'the opponent in the banja luka tournament was carlos berlocq with a score of 4 - 6 , 4 - 6')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-17344918-1.html.csv', '3 - 0 was the 2nd leg when san pedro was the 2nd place winner')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12512153-4.html.csv', 'pat perez placed lower than health slocum with a score of 67')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('1-10953197-4.html.csv', 'frank south directed all the episodes till full metal betsu')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-15532127-1.html.csv', 'on june 1st , 1997 , there were 3 competitions with a result of 5 - 0 and the scores were 1 - 0 , 3 - 0 , and 4 - 0')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-12805568-1.html.csv', 'the reactortype vver - 365 (prototype) had a gross capacity of 1000 mw and was shutdown on 29.08.1990')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-11780179-1.html.csv', 'uzbekistan is the country that has a gdp (nominal) less than $29.9 billion')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1007688-1.html.csv', 'in 1935 , when there were 4 cases of smallpox , there were 140 cases of typhus')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-29728787-1.html.csv', 'karl ridderbusch was the la roche in a year after 1972')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-20626467-1.html.csv', 'the result for the time of 1 - 26.21 was 1st on 20 / 09 / 08')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-16013858-11.html.csv', 'on june 26 the score in set 4 was 15 - 13')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-10883333-13.html.csv', 'essendon scored 18.18 (132)')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-11872185-5.html.csv', 'in 2010 , excel entertainment was the studio for dabangg , which had an opening day net gross of 14 , 45 , 00000')\n",
      "['Contradictory', 'Contradictory', 'Neutral']\n",
      "('2-17162128-3.html.csv', 'steve jones won in multiple years with a 154 total')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1341640-14.html.csv', 'cardiss collins was first elected as democrat in 1969')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-21995420-9.html.csv', 'if the school is far eastern university feu cheering squad , then the tumbling number is 49')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-2509350-3.html.csv', 'the language for sopachuy 10 is quechua')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-14319023-2.html.csv', 'in 2009 , when the girls doubles is anneke feinya agustin and wenny setiawati , the mixed doubles is wifqi windarto and debby susanto')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12805568-1.html.csv', \"the vver - 440 / 179 's gross capacity is 537 mw while the electricity grid for it is 32.58.4568\")\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-14347546-5.html.csv', 'the record for the game with under 40 points , over game 25 and with a 2 - 0 score is 15 - 7 - 7 - 1')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-16965329-5.html.csv', 'brasov tournament was not against daniel eisner')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('1-23696862-6.html.csv', 'the greatest amount of wsop bracelets anyone had is zero')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-17327458-1.html.csv', 'derek ferguson was sacked on january 24th')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('2-184391-1.html.csv', 'slovenia was adopted in 1999')\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-184391-1.html.csv', 'for adopted 1999 - 01 - 01 , the state of portugal has a relative gdp of total (nominal) 1.83% which is lower than the relative gdp of total (nominal) for ireland with the same adopted date')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('2-17358229-1.html.csv', 'the world race walking cup held after 2001 in ppodebrady , czech republic and m\\xc3\\xa9zidon - canon , france')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-18281308-2.html.csv', 'kenneth allen won his match against mark bear due to tko (cut) at ifc : rumble on the river')\n",
      "['Contradictory', 'Neutral', 'Entailed']\n",
      "('2-1330308-2.html.csv', \"opponent yurij kiselov 's time was greater than 1:30\")\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-14347546-5.html.csv', 'the florida panthers are the opponents in game 31')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1028356-3.html.csv', '4 - 6 , 6 - 2 , 6 - 3 is the score with partner sherwood stewart')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-12552861-1.html.csv', \"when sue devlin won the women 's singles in 1957 , robert b. williams ethel marshall won the mixed doubles\")\n",
      "['Contradictory', 'Contradictory', 'Entailed']\n",
      "('2-17344918-1.html.csv', '4 - 1 was the 1st leg for 2004')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-27821519-1.html.csv', 'mike bliss had 22 top tens')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1028356-3.html.csv', 'one match at wimbledon resulted in the score : 3 - 6 , 7 - 6 (5) , 6 - 3')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('1-1028356-3.html.csv', 'it was after the year 1990 when one match was played with jim pugh')\n",
      "['Contradictory', 'Contradictory', 'Contradictory']\n",
      "('2-16013858-11.html.csv', 'set 4 is na when the date before jun 27th and set 3 after 17 - 25')\n",
      "['Contradictory', 'Entailed', 'Entailed']\n",
      "('1-2509350-3.html.csv', 'the most padilla municipality for guarani is 2181')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n",
      "('1-10953197-4.html.csv', 'the number of episodes with a production code of 2395118 is number 26.0')\n",
      "['Contradictory', 'Entailed', 'Contradictory']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from collections import Counter\n",
    "import pprint\n",
    "\n",
    "t = pandas.read_csv(\"/Users/wenhuchen/Downloads/Batch_3598504_batch_results.csv\")\n",
    "t = t[t.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "\n",
    "def transform(string):\n",
    "    if string == \"Problematic\":\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "kv_pairs = {}\n",
    "num = 5\n",
    "for i, r in t.iterrows():\n",
    "    for j in range(1, num + 1):\n",
    "        csv_name = r['Input.url{}'.format(j)].split('/')[-1]\n",
    "        if (csv_name, r['Input.s{}'.format(j)]) not in kv_pairs:\n",
    "            kv_pairs[(csv_name, r['Input.s{}'.format(j)])] = [r[\"Input.o{}\".format(j)], r[\"Answer.A{}\".format(j)], \"None\"]\n",
    "        else:\n",
    "            kv_pairs[(csv_name, r['Input.s{}'.format(j)])][2] = r[\"Answer.A{}\".format(j)]\n",
    "\n",
    "#new_kv_pairs = {}\n",
    "#for k, v in kv_pairs.iteritems():\n",
    "#    if \"Problematic\" not in v:\n",
    "#        new_kv_pairs[k] = v\n",
    "        #if v[1] == v[2]:\n",
    "        #    new_kv_pairs[k][0] = v[1]\n",
    "            \n",
    "counter = Counter()\n",
    "p_c = [] \n",
    "for k, v in kv_pairs.iteritems():\n",
    "    tmp = set(v)\n",
    "    if len(tmp) == 1:\n",
    "        p_c.append(1)\n",
    "    elif len(tmp) == 2:\n",
    "        p_c.append(1/3.)\n",
    "    else:\n",
    "        p_c.append(0)\n",
    "    \n",
    "    counter.update(v)\n",
    "\n",
    "p_e = []\n",
    "for i, j in counter.items():\n",
    "    p_e.append(j + 0.0)\n",
    "\n",
    "p_e = [_ / sum(p_e) for _ in p_e]\n",
    "p_e = sum(_**2 for _ in p_e)\n",
    "\n",
    "p_c = sum(p_c) / len(p_c)\n",
    "\n",
    "kappa = (p_c - p_e) / (1 - p_e)\n",
    "print \"kappa = {}\".format(kappa)\n",
    "print counter\n",
    "for k, v in new_kv_pairs.iteritems():\n",
    "    if v[0] == 'Contradictory':\n",
    "        print k\n",
    "        print v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pairs = {}\n",
    "\n",
    "def decide(s1, s2, s3):\n",
    "    s2int = {'Entailed': 1, 'Neutral': 0, 'Contradictory': -1}\n",
    "    avg_score = (s2int[s1] + s2int[s2] + s2int[s3]) // 3.\n",
    "    if avg_score > 0:\n",
    "        return \"Entailed\"\n",
    "    elif avg_score < 0:\n",
    "        return \"Contradictory\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "for k, v in new_kv_pairs.iteritems():\n",
    "    if k[0] not in cleaned_pairs:\n",
    "        cleaned_pairs[k[0]] = [[k[1]], [decide(*v)]]\n",
    "    else:\n",
    "        cleaned_pairs[k[0]][0].append(k[1])\n",
    "        cleaned_pairs[k[0]][1].append(decide(*v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('READY/cleaned.json', 'w') as f:\n",
    "    json.dump(cleaned_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "\n",
    "with open('READY/training_all.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('data/table_to_page.json') as f:\n",
    "    mapping = json.load(f)\n",
    "\n",
    "dummy = [\n",
    "    # supported\n",
    "    [\n",
    "        'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/2-18847467-2.html.csv',\n",
    "        'the total attendance for week 8 is 48667',\n",
    "        '1982 new orleans saints season',\n",
    "        'https://en.wikipedia.org/wiki/1982_New_Orleans_Saints_season'\n",
    "    ],\n",
    "    # refuted\n",
    "    [\n",
    "        'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/2-18847467-2.html.csv',\n",
    "        'chicago bears has won the game on september',\n",
    "        '1982 new orleans saints season',\n",
    "        'https://en.wikipedia.org/wiki/1982_New_Orleans_Saints_season'\n",
    "    ],\n",
    "    [\n",
    "        'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/2-18847467-2.html.csv',\n",
    "        'chicago bears is the best team in the league',\n",
    "        '1982 new orleans saints season',\n",
    "        'https://en.wikipedia.org/wiki/1982_New_Orleans_Saints_season'\n",
    "    ],\n",
    "    # erroneous\n",
    "    [\n",
    "        'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/2-18847467-2.html.csv',\n",
    "        'the ninth week against atlant falcons was a big win with 35 - 6',\n",
    "        '1982 new orleans saints season',\n",
    "        'https://en.wikipedia.org/wiki/1982_New_Orleans_Saints_season'\n",
    "    ],\n",
    "    [\n",
    "        'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/2-18847467-2.html.csv',\n",
    "        'the result against st. lous cardinals was a lost',\n",
    "        '1982 new orleans saints season',\n",
    "        'https://en.wikipedia.org/wiki/1982_New_Orleans_Saints_season'\n",
    "    ],\n",
    "    [\n",
    "        'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/2-18847467-2.html.csv',\n",
    "        'san francisco 49ers was competing on noveber 28 , 1982',\n",
    "        '1982 new orleans saints season',\n",
    "        'https://en.wikipedia.org/wiki/1982_New_Orleans_Saints_season'\n",
    "    ],\n",
    "        [\n",
    "        'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/2-18847467-2.html.csv',\n",
    "        'san francisco is is opponent where week is number',\n",
    "        '1982 new orleans saints season',\n",
    "        'https://en.wikipedia.org/wiki/1982_New_Orleans_Saints_season'\n",
    "    ]\n",
    "]\n",
    "    \n",
    "num = 10\n",
    "count = 0\n",
    "with open('verify_inputs_fake.csv', 'w') as fs:\n",
    "    fields = []\n",
    "    for i in range(0, num + 1):\n",
    "        fields.extend(['wiki{}'.format(i), 'url{}'.format(i), 's{}'.format(i), 'topic{}'.format(i)])\n",
    "    csvwriter = csv.DictWriter(fs, fieldnames=fields)\n",
    "    csvwriter.writeheader()\n",
    "\n",
    "    seed = random.randint(0, len(dummy) - 1)\n",
    "    buf = {'wiki0': dummy[seed][3], 'topic0': dummy[seed][2], 'url0': dummy[seed][0], 's0': dummy[seed][1]}\n",
    "    for k, v in data.iteritems():\n",
    "        entry = data[k]\n",
    "        for sent, lab in zip(entry[0], entry[1]):\n",
    "            if lab == 0:\n",
    "                cur = len(buf) // 4\n",
    "                if 'url10' in buf:\n",
    "                    csvwriter.writerow(buf)\n",
    "                    seed = random.randint(0, len(dummy) - 1)\n",
    "                    buf = {'wiki0': dummy[seed][3], 'topic0': dummy[seed][2], 'url0': dummy[seed][0], 's0': dummy[seed][1]}\n",
    "                    cur = 1\n",
    "                    count += 1\n",
    "                buf['url{}'.format(cur)] = 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k\n",
    "                buf['s{}'.format(cur)] = sent\n",
    "                if k == mapping:\n",
    "                    buf['wiki{}'.format(cur)] = mapping[k][1]\n",
    "                    buf['topic{}'.format(cur)] = mapping[k][0]\n",
    "                else:\n",
    "                    buf['wiki{}'.format(cur)] = \"#\"\n",
    "                    buf['topic{}'.format(cur)] = \"None\"\n",
    "                    \n",
    "        #if count > 30:\n",
    "        #    break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import json\n",
    "import csv\n",
    "\n",
    "with open('READY/training_all.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "num = 11\n",
    "count = 0\n",
    "with open('verify_inputs_fake.csv', 'w') as fs:\n",
    "    fields = []\n",
    "    for i in range(0, num):\n",
    "        fields.extend(['wiki{}'.format(i), 'url{}'.format(i), 's{}'.format(i), 'o{}'.format(i)])\n",
    "    csvwriter = csv.DictWriter(fs, fieldnames=fields)\n",
    "    csvwriter.writeheader()\n",
    "\n",
    "    buf = {}\n",
    "    for k, v in data.iteritems():\n",
    "        entry = data[k]\n",
    "        for sent, lab in zip(entry[0], entry[1]):\n",
    "            cur = len(buf) // 3 + 1\n",
    "            if cur > num:\n",
    "                csvwriter.writerow(buf)\n",
    "                buf = {}\n",
    "                cur = 1\n",
    "                count += 1\n",
    "            buf['url{}'.format(cur)] = 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k\n",
    "            buf['s{}'.format(cur)] = sent\n",
    "            if lab == 1:\n",
    "                buf['o{}'.format(cur)] = \"Entailed\"\n",
    "            else:\n",
    "                buf['o{}'.format(cur)] = \"Contradictory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('all_sources/full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "    #mapping[str(d['goldAnnotation']['titleId']) + '-' + str(d['tableId'])] = d['goldAnnotation']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for d in data:\n",
    "    mapping[str(d['pgId']) + '-' + str(d['tableId'])] = d['pgTitle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "tiny_mapping = {}\n",
    "for f in os.listdir('data/all_csv/'):\n",
    "    if f.endswith('.csv'):\n",
    "        _, pageid, tableid = f.split('.')[0].split('-')\n",
    "        if pageid + '-' + tableid in mapping:\n",
    "            tiny_mapping[f] = \"https://en.wikipedia.org/wiki/\" + \"_\".join(mapping[pageid + '-' + tableid].split(' '))\n",
    "\n",
    "with open('data/table_to_page.json', 'w') as f:\n",
    "    json.dump(tiny_mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/table_to_page.json', 'r') as f:\n",
    "    tiny_mapping = json.load(f)\n",
    "\n",
    "print len(tiny_mapping)\n",
    "print tiny_mapping['1-18974269-1.html.csv']\n",
    "#print tiny_mapping['1-1007688-1.html.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "with open('data/table_to_page.json', 'r') as f:\n",
    "    tiny_mapping = json.load(f)\n",
    "    \n",
    "new_tiny_mapping = {}\n",
    "for k, v in tiny_mapping.iteritems():\n",
    "    new_tiny_mapping[k] = \"https://en.wikipedia.org/wiki/\" + urllib.quote(v[30:].encode('utf8'))\n",
    "\n",
    "with open('data/table_to_page_new.json', 'w') as f:\n",
    "    json.dump(new_tiny_mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "t = pandas.read_csv('v5_write_input.csv')\n",
    "length = len(t)\n",
    "\n",
    "t = t.head(3000).tail(2000)\n",
    "#t = t.head(1000)\n",
    "\n",
    "t.to_csv('v5_write_input_1000_3000.csv', index=False)\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "t = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest-v2/first1000.csv\")\n",
    "print len(t)\n",
    "t = t[t.AssignmentStatus==\"Approved\"]\n",
    "print len(t)\n",
    "index = 0\n",
    "num = 5\n",
    "finished = {}\n",
    "for i,r in t.iterrows():\n",
    "    html_name = r['Input.url1']\n",
    "    html_name = html_name.split('/')[-1]\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        #if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "        #    print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3:\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            #replaced_sent = orig_input\n",
    "            index += 1\n",
    "            if html_name not in finished:\n",
    "                finished[html_name] = [[replaced_sent], [1]]\n",
    "            else:\n",
    "                finished[html_name][0].append(replaced_sent)\n",
    "                finished[html_name][1].append(1)\n",
    "\n",
    "with open(\"READY/round2_first1000.json\",'w') as f:\n",
    "    json.dump(finished, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "t = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest-v2/last4000.csv\")\n",
    "print len(t)\n",
    "t = t[t.AssignmentStatus==\"Approved\"]\n",
    "print len(t)\n",
    "index = 0\n",
    "num = 5\n",
    "finished = {}\n",
    "for i,r in t.iterrows():\n",
    "    html_name = r['Input.url1']\n",
    "    html_name = html_name.split('/')[-1]\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        #if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "        #    print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3:\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            #replaced_sent = orig_input\n",
    "            index += 1\n",
    "            if html_name not in finished:\n",
    "                finished[html_name] = [[replaced_sent], [1]]\n",
    "            else:\n",
    "                finished[html_name][0].append(replaced_sent)\n",
    "                finished[html_name][1].append(1)\n",
    "\n",
    "with open(\"READY/round2_last4000.json\",'w') as f:\n",
    "    json.dump(finished, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "t = pandas.read_csv(\"/Users/wenhuchen/Downloads/harvest-v2/middle2000.csv\")\n",
    "print len(t)\n",
    "t = t[t.AssignmentStatus==\"Approved\"]\n",
    "print len(t)\n",
    "index = 0\n",
    "num = 5\n",
    "finished = {}\n",
    "for i,r in t.iterrows():\n",
    "    html_name = r['Input.url1']\n",
    "    html_name = html_name.split('/')[-1]\n",
    "    for j in range(1, num + 1):\n",
    "        orig_input = r[\"Answer.d{}\".format(j)]\n",
    "        #if isinstance(orig_input, str) and orig_input.lower() in [\"na\", \"n/a\", \"no\"]:\n",
    "        #    print \"error\"\n",
    "        if isinstance(orig_input, str) and len(orig_input.split(' ')) > 3:\n",
    "            replaced_sent = substitute(orig_input)\n",
    "            #replaced_sent = orig_input\n",
    "            index += 1\n",
    "            if html_name not in finished:\n",
    "                finished[html_name] = [[replaced_sent], [1]]\n",
    "            else:\n",
    "                finished[html_name][0].append(replaced_sent)\n",
    "                finished[html_name][1].append(1)\n",
    "\n",
    "with open(\"READY/round2_middle2000.json\",'w') as f:\n",
    "    json.dump(finished, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import csv\n",
    "\n",
    "with open(\"READY/round2_first1000.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"data/table_to_page.json\") as f:\n",
    "    mapping = json.load(f)\n",
    "    \n",
    "fields = ['url1', 'wiki1', 'topic1', 's1', 's2', 's3', 's4', 's5']\n",
    "index = 0\n",
    "with open(\"rewrite_fake_first1000.csv\", 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for k, v in data.iteritems():\n",
    "        for i in range(0, len(v[0]), 5):\n",
    "            v_s = v[0][i:i+5]\n",
    "            field = {}\n",
    "            for j, s in enumerate(v_s):\n",
    "                field['s{}'.format(j + 1)] = s\n",
    "            field['url1'] = 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k\n",
    "            field['wiki1'] = mapping[k][1]\n",
    "            field['topic1'] = mapping[k][0]\n",
    "            writer.writerow(field)\n",
    "            index += 1\n",
    "        #if index > 20:\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import csv\n",
    "\n",
    "with open(\"READY/round2_middle2000.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"data/table_to_page.json\") as f:\n",
    "    mapping = json.load(f)\n",
    "    \n",
    "fields = ['url1', 'wiki1', 'topic1', 's1', 's2', 's3', 's4', 's5']\n",
    "index = 0\n",
    "with open(\"rewrite_fake_middle2000.csv\", 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for k, v in data.iteritems():\n",
    "        for i in range(0, len(v[0]), 5):\n",
    "            v_s = v[0][i:i+5]\n",
    "            field = {}\n",
    "            for j, s in enumerate(v_s):\n",
    "                field['s{}'.format(j + 1)] = s\n",
    "            field['url1'] = 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k\n",
    "            field['wiki1'] = mapping[k][1]\n",
    "            field['topic1'] = mapping[k][0]\n",
    "            writer.writerow(field)\n",
    "            index += 1\n",
    "        #if index > 20:\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "import csv\n",
    "\n",
    "with open(\"READY/round2_last4000.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "files = data.keys()\n",
    "num = 1473\n",
    "shiyang_data = files[:num]\n",
    "wanghong_data = files[num:2*num]\n",
    "yunkai_data = files[2*num:]\n",
    "    \n",
    "with open(\"data/table_to_page.json\") as f:\n",
    "    mapping = json.load(f)\n",
    "    \n",
    "fields = ['url1', 'wiki1', 'topic1', 's1', 's2', 's3', 's4', 's5']\n",
    "\n",
    "with open(\"rewrite_fake_shiyang.csv\", 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for k in shiyang_data:\n",
    "        v = data[k]\n",
    "        for i in range(0, len(v[0]), 5):\n",
    "            v_s = v[0][i:i+5]\n",
    "            field = {}\n",
    "            for j, s in enumerate(v_s):\n",
    "                field['s{}'.format(j + 1)] = s\n",
    "            field['url1'] = 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k\n",
    "            field['wiki1'] = mapping[k][1]\n",
    "            field['topic1'] = mapping[k][0]\n",
    "            writer.writerow(field)\n",
    "            \n",
    "with open(\"rewrite_fake_yunkai.csv\", 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for k in yunkai_data:\n",
    "        v = data[k]\n",
    "        for i in range(0, len(v[0]), 5):\n",
    "            v_s = v[0][i:i+5]\n",
    "            field = {}\n",
    "            for j, s in enumerate(v_s):\n",
    "                field['s{}'.format(j + 1)] = s\n",
    "            field['url1'] = 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k\n",
    "            field['wiki1'] = mapping[k][1]\n",
    "            field['topic1'] = mapping[k][0]\n",
    "            writer.writerow(field)\n",
    "            \n",
    "with open(\"rewrite_fake_wanghong.csv\", 'w') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    for k in wanghong_data:\n",
    "        v = data[k]\n",
    "        for i in range(0, len(v[0]), 5):\n",
    "            v_s = v[0][i:i+5]\n",
    "            field = {}\n",
    "            for j, s in enumerate(v_s):\n",
    "                field['s{}'.format(j + 1)] = s\n",
    "            field['url1'] = 'https://raw.githubusercontent.com/wenhuchen/Table-Fact-Checking/master/data/all_csv/' + k\n",
    "            field['wiki1'] = mapping[k][1]\n",
    "            field['topic1'] = mapping[k][0]\n",
    "            writer.writerow(field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "t = pandas.read_csv('/Users/wenhuchen/Downloads/harvest-v3/partial_positive.csv')\n",
    "t = t[t.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "mapping = {}\n",
    "for i, r in t.iterrows():\n",
    "    for i in range(1, 6):\n",
    "        name = r['Input.url{}'.format(i)].split('/')[-1]\n",
    "        if name not in mapping:\n",
    "            mapping[name] = [[r['Input.s{}'.format(i)]], [r['Answer.A{}'.format(i)]]]\n",
    "        else:\n",
    "            mapping[name][0].append(r['Input.s{}'.format(i)])\n",
    "            mapping[name][1].append(r['Answer.A{}'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('READY/verify.json', 'w') as f:\n",
    "    json.dump(mapping, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "t = pandas.read_csv('/Users/wenhuchen/Downloads/clean/positive.csv')\n",
    "t = t[t.AssignmentStatus==\"Approved\"]\n",
    "\n",
    "print len(t) * 10\n",
    "num = 10\n",
    "positive = {}\n",
    "count = 0\n",
    "for i, row in t.iterrows():\n",
    "    for j in range(1, num + 1):\n",
    "        if row['Answer.A{}'.format(j)] == \"Entailed\":\n",
    "            name = row['Input.url{}'.format(j)].split('/')[-1]\n",
    "            count += 1\n",
    "            if name in positive:\n",
    "                positive[name][0].append(row['Input.s{}'.format(j)])\n",
    "                positive[name][1].append(1)\n",
    "            else:\n",
    "                positive[name] = [[row['Input.s{}'.format(j)]], [1]]\n",
    "\n",
    "print count\n",
    "with open('READY/cleaned_positive.json', 'w') as f:\n",
    "    json.dump(positive, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
